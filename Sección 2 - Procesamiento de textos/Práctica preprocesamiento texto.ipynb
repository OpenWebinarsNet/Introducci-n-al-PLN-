{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPb4YaFS+IlmCZ2x0dIadFZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Preprocesamiento de textos\n","\n","Como hemos visto existen una serie de técnicas que mejoran los resultados de la distintas tareas de Procesamiento del Lenguaje Natural.\n","\n","En esta práctica y apoyándonos en el módulo `nltk` veremos cómo preparar un texto para su posterior análisis llevando a cabo las siguientes tareas:\n","\n","1. Tokenización\n","2. Eliminación de mayúsculas\n","3. Eliminación de caracteres alfabéticos\n","4. Eliminación de palabras vacías\n","5. Stemming\n","\n","\n","\n","Comenzamos importando los módulos que vamos a utilizar:\n"],"metadata":{"id":"x_zJAIbOKOtj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"mkACr2wgKHeW","executionInfo":{"status":"ok","timestamp":1709751867471,"user_tz":-60,"elapsed":1656,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}}},"outputs":[],"source":["import nltk"]},{"cell_type":"markdown","source":["El módulo `nltk` incluye además de muchas funciones destinadas al PLN un conjunto de datos compuestos por distintos libros que podemos utilizar para nuestras primeras pruebas de PLN.\n","\n","Veámos de que libros dispone:"],"metadata":{"id":"wh2HASRmLIo9"}},{"cell_type":"code","source":["nltk.download()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3P15BO7KsVe","executionInfo":{"status":"ok","timestamp":1709751928087,"user_tz":-60,"elapsed":39732,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"954ff2c4-5284-4a5e-a38b-f574c31be82a"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> l\n","\n","Packages:\n","  [ ] abc................. Australian Broadcasting Commission 2006\n","  [ ] alpino.............. Alpino Dutch Treebank\n","  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n","  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n","  [ ] basque_grammars..... Grammars for Basque\n","  [ ] bcp47............... BCP-47 Language Tags\n","  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n","                           Extraction Systems in Biology)\n","  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n","  [ ] book_grammars....... Grammars from NLTK Book\n","  [ ] brown............... Brown Corpus\n","  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n","  [ ] cess_cat............ CESS-CAT Treebank\n","  [ ] cess_esp............ CESS-ESP Treebank\n","  [ ] chat80.............. Chat-80 Data Files\n","  [ ] city_database....... City Database\n","  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n","  [ ] comparative_sentences Comparative Sentence Dataset\n","  [ ] comtrans............ ComTrans Corpus Sample\n","  [ ] conll2000........... CONLL 2000 Chunking Corpus\n","Hit Enter to continue: \n","  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n","  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n","                           and Basque Subset)\n","  [ ] crubadan............ Crubadan Corpus\n","  [ ] dependency_treebank. Dependency Parsed Treebank\n","  [ ] dolch............... Dolch Word List\n","  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n","                           Corpus\n","  [ ] extended_omw........ Extended Open Multilingual WordNet\n","  [ ] floresta............ Portuguese Treebank\n","  [ ] framenet_v15........ FrameNet 1.5\n","  [ ] framenet_v17........ FrameNet 1.7\n","  [ ] gazetteers.......... Gazeteer Lists\n","  [ ] genesis............. Genesis Corpus\n","  [ ] gutenberg........... Project Gutenberg Selections\n","  [ ] ieer................ NIST IE-ER DATA SAMPLE\n","  [ ] inaugural........... C-Span Inaugural Address Corpus\n","  [ ] indian.............. Indian Language POS-Tagged Corpus\n","  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n","                           ChaSen format)\n","  [ ] kimmo............... PC-KIMMO Data Files\n","Hit Enter to continue: \n","  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n","  [ ] large_grammars...... Large context-free and feature-based grammars\n","                           for parser comparison\n","  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n","  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n","                           part-of-speech tags\n","  [ ] machado............. Machado de Assis -- Obra Completa\n","  [ ] masc_tagged......... MASC Tagged Corpus\n","  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n","  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n","  [ ] moses_sample........ Moses Sample Models\n","  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n","  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n","  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n","                           2015) subset of the Paraphrase Database.\n","  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n","  [ ] nombank.1.0......... NomBank Corpus 1.0\n","  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n","  [ ] nps_chat............ NPS Chat\n","  [ ] omw-1.4............. Open Multilingual Wordnet\n","  [ ] omw................. Open Multilingual Wordnet\n","Hit Enter to continue: \n","  [ ] opinion_lexicon..... Opinion Lexicon\n","  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n","  [ ] paradigms........... Paradigm Corpus\n","  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n","                           Evaluation Shared Task\n","  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n","                           character properties in Perl\n","  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n","  [ ] pl196x.............. Polish language of the XX century sixties\n","  [ ] porter_test......... Porter Stemmer Test Files\n","  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n","  [ ] problem_reports..... Problem Report Corpus\n","  [ ] product_reviews_1... Product Reviews (5 Products)\n","  [ ] product_reviews_2... Product Reviews (9 Products)\n","  [ ] propbank............ Proposition Bank Corpus 1.0\n","  [ ] pros_cons........... Pros and Cons\n","  [ ] ptb................. Penn Treebank\n","  [ ] punkt............... Punkt Tokenizer Models\n","  [ ] qc.................. Experimental Data for Question Classification\n","  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n","                           version\n","Hit Enter to continue: \n","  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n","                           Portuguesa)\n","  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n","  [ ] sample_grammars..... Sample Grammars\n","  [ ] semcor.............. SemCor 3.0\n","  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n","  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n","  [ ] sentiwordnet........ SentiWordNet\n","  [ ] shakespeare......... Shakespeare XML Corpus Sample\n","  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n","  [ ] smultron............ SMULTRON Corpus Sample\n","  [ ] snowball_data....... Snowball Data\n","  [ ] spanish_grammars.... Grammars for Spanish\n","  [ ] state_union......... C-Span State of the Union Address Corpus\n","  [ ] stopwords........... Stopwords Corpus\n","  [ ] subjectivity........ Subjectivity Dataset v1.0\n","  [ ] swadesh............. Swadesh Wordlists\n","  [ ] switchboard......... Switchboard Corpus Sample\n","  [ ] tagsets............. Help on Tagsets\n","  [ ] timit............... TIMIT Corpus Sample\n","  [ ] toolbox............. Toolbox Sample Files\n","Hit Enter to continue: \n","  [ ] treebank............ Penn Treebank Sample\n","  [ ] twitter_samples..... Twitter Samples\n","  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n","                           (Unicode Version)\n","  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","  [ ] unicode_samples..... Unicode Samples\n","  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n","  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n","  [ ] vader_lexicon....... VADER Sentiment Lexicon\n","  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n","  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n","  [ ] webtext............. Web Text Corpus\n","  [ ] wmt15_eval.......... Evaluation data from WMT15\n","  [ ] word2vec_sample..... Word2Vec Sample\n","  [ ] wordnet2021......... Open English Wordnet 2021\n","  [ ] wordnet2022......... Open English Wordnet 2022\n","  [ ] wordnet31........... Wordnet 3.1\n","  [ ] wordnet............. WordNet\n","  [ ] wordnet_ic.......... WordNet-InfoContent\n","  [ ] words............... Word Lists\n","  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n","                           English Prose\n","Hit Enter to continue: \n","\n","Collections:\n","  [ ] all-corpora......... All the corpora\n","  [ ] all-nltk............ All packages available on nltk_data gh-pages\n","                           branch\n","  [ ] all................. All packages\n","  [ ] book................ Everything used in the NLTK Book\n","  [ ] popular............. Popular packages\n","  [ ] tests............... Packages for running tests\n","  [ ] third-party......... Third-party data packages\n","\n","([*] marks installed packages)\n","\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader> q\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["nltk.corpus.gutenberg.fileids()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_Oro6THMNKF","executionInfo":{"status":"ok","timestamp":1709751965569,"user_tz":-60,"elapsed":3,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"7d02a983-be7f-49f8-c143-263cd1cebbcb"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['austen-emma.txt',\n"," 'austen-persuasion.txt',\n"," 'austen-sense.txt',\n"," 'bible-kjv.txt',\n"," 'blake-poems.txt',\n"," 'bryant-stories.txt',\n"," 'burgess-busterbrown.txt',\n"," 'carroll-alice.txt',\n"," 'chesterton-ball.txt',\n"," 'chesterton-brown.txt',\n"," 'chesterton-thursday.txt',\n"," 'edgeworth-parents.txt',\n"," 'melville-moby_dick.txt',\n"," 'milton-paradise.txt',\n"," 'shakespeare-caesar.txt',\n"," 'shakespeare-hamlet.txt',\n"," 'shakespeare-macbeth.txt',\n"," 'whitman-leaves.txt']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["nltk.download('gutenberg')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HigwLI1tKFwq","executionInfo":{"status":"ok","timestamp":1709751954099,"user_tz":-60,"elapsed":732,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"c0e86b3d-748e-4409-afaf-91e79f0add0d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Estos son solo algunos ejemplos de textos de fácil acceso para probar temas de PLN. En nuestro caso y para que el ejemplo pueda ser seguido por todos buscaremos un fragmento de un libro en español."],"metadata":{"id":"0wLLvYoPOAMx"}},{"cell_type":"code","source":["don_quijote = \"\"\"\n","Capítulo primero. Que trata de la condición y ejercicio del famoso hidalgo\n","don Quijote de la Mancha\n","\n","En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\n","tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n","rocín flaco y galgo corredor. Una olla de algo más vaca que carnero,\n","salpicón las más noches, duelos y quebrantos los sábados, lantejas los\n","viernes, algún palomino de añadidura los domingos, consumían las tres\n","partes de su hacienda. El resto della concluían sayo de velarte, calzas de\n","velludo para las fiestas, con sus pantuflos de lo mesmo, y los días de\n","entresemana se honraba con su vellorí de lo más fino. Tenía en su casa una\n","ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte,\n","y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la\n","podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años; era de\n","complexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo\n","de la caza. Quieren decir que tenía el sobrenombre de Quijada, o Quesada,\n","que en esto hay alguna diferencia en los autores que deste caso escriben;\n","aunque, por conjeturas verosímiles, se deja entender que se llamaba\n","Quejana. Pero esto importa poco a nuestro cuento; basta que en la narración\n","dél no se salga un punto de la verdad.\n","\n","Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba\n","ocioso, que eran los más del año, se daba a leer libros de caballerías, con\n","tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la\n","caza, y aun la administración de su hacienda. Y llegó a tanto su curiosidad\n","y desatino en esto, que vendió muchas hanegas de tierra de sembradura para\n","comprar libros de caballerías en que leer, y así, llevó a su casa todos\n","cuantos pudo haber dellos; y de todos, ningunos le parecían tan bien como\n","los que compuso el famoso Feliciano de Silva, porque la claridad de su\n","prosa y aquellas entricadas razones suyas le parecían de perlas, y más\n","cuando llegaba a leer aquellos requiebros y cartas de desafíos, donde en\n","muchas partes hallaba escrito: La razón de la sinrazón que a mi razón se\n","hace, de tal manera mi razón enflaquece, que con razón me quejo de la\n","vuestra fermosura. Y también cuando leía: ...los altos cielos que de\n","vuestra divinidad divinamente con las estrellas os fortifican, y os hacen\n","merecedora del merecimiento que merece la vuestra grandeza.\n","\n","Con estas razones perdía el pobre caballero el juicio, y desvelábase por\n","entenderlas y desentrañarles el sentido, que no se lo sacara ni las\n","entendiera el mesmo Aristóteles, si resucitara para sólo ello. No estaba\n","muy bien con las heridas que don Belianís daba y recebía, porque se\n","imaginaba que, por grandes maestros que le hubiesen curado, no dejaría de\n","tener el rostro y todo el cuerpo lleno de cicatrices y señales. Pero, con\n","todo, alababa en su autor aquel acabar su libro con la promesa de aquella\n","inacabable aventura, y muchas veces le vino deseo de tomar la pluma y dalle\n","fin al pie de la letra, como allí se promete; y sin duda alguna lo hiciera,\n","y aun saliera con ello, si otros mayores y continuos pensamientos no se lo\n","estorbaran. Tuvo muchas veces competencia con el cura de su lugar —que era\n","hombre docto, graduado en Sigüenza—, sobre cuál había sido mejor caballero:\n","Palmerín de Ingalaterra o Amadís de Gaula; mas maese Nicolás, barbero del\n","mesmo pueblo, decía que ninguno llegaba al Caballero del Febo, y que si\n","alguno se le podía comparar, era don Galaor, hermano de Amadís de Gaula,\n","porque tenía muy acomodada condición para todo; que no era caballero\n","melindroso, ni tan llorón como su hermano, y que en lo de la valentía no le\n","iba en zaga.\n","\"\"\""],"metadata":{"id":"-EzC-htgLc0v","executionInfo":{"status":"ok","timestamp":1709752005301,"user_tz":-60,"elapsed":868,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Al visualizar el texto observamos que tenemos signos de espaciado y salto de página, puntuación, tildes...\n","\n","Veámos cómo podemos preprocesar este texto:"],"metadata":{"id":"7f_zHVaOPw2I"}},{"cell_type":"code","source":["don_quijote"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"mI0Xe6xKOSQ8","executionInfo":{"status":"ok","timestamp":1709752011854,"user_tz":-60,"elapsed":415,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"b5a79096-2ca8-4dd9-a58d-da40badf28e8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nCapítulo primero. Que trata de la condición y ejercicio del famoso hidalgo\\ndon Quijote de la Mancha\\n\\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho\\ntiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\\nrocín flaco y galgo corredor. Una olla de algo más vaca que carnero,\\nsalpicón las más noches, duelos y quebrantos los sábados, lantejas los\\nviernes, algún palomino de añadidura los domingos, consumían las tres\\npartes de su hacienda. El resto della concluían sayo de velarte, calzas de\\nvelludo para las fiestas, con sus pantuflos de lo mesmo, y los días de\\nentresemana se honraba con su vellorí de lo más fino. Tenía en su casa una\\nama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte,\\ny un mozo de campo y plaza, que así ensillaba el rocín como tomaba la\\npodadera. Frisaba la edad de nuestro hidalgo con los cincuenta años; era de\\ncomplexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo\\nde la caza. Quieren decir que tenía el sobrenombre de Quijada, o Quesada,\\nque en esto hay alguna diferencia en los autores que deste caso escriben;\\naunque, por conjeturas verosímiles, se deja entender que se llamaba\\nQuejana. Pero esto importa poco a nuestro cuento; basta que en la narración\\ndél no se salga un punto de la verdad.\\n\\nEs, pues, de saber que este sobredicho hidalgo, los ratos que estaba\\nocioso, que eran los más del año, se daba a leer libros de caballerías, con\\ntanta afición y gusto, que olvidó casi de todo punto el ejercicio de la\\ncaza, y aun la administración de su hacienda. Y llegó a tanto su curiosidad\\ny desatino en esto, que vendió muchas hanegas de tierra de sembradura para\\ncomprar libros de caballerías en que leer, y así, llevó a su casa todos\\ncuantos pudo haber dellos; y de todos, ningunos le parecían tan bien como\\nlos que compuso el famoso Feliciano de Silva, porque la claridad de su\\nprosa y aquellas entricadas razones suyas le parecían de perlas, y más\\ncuando llegaba a leer aquellos requiebros y cartas de desafíos, donde en\\nmuchas partes hallaba escrito: La razón de la sinrazón que a mi razón se\\nhace, de tal manera mi razón enflaquece, que con razón me quejo de la\\nvuestra fermosura. Y también cuando leía: ...los altos cielos que de\\nvuestra divinidad divinamente con las estrellas os fortifican, y os hacen\\nmerecedora del merecimiento que merece la vuestra grandeza.\\n\\nCon estas razones perdía el pobre caballero el juicio, y desvelábase por\\nentenderlas y desentrañarles el sentido, que no se lo sacara ni las\\nentendiera el mesmo Aristóteles, si resucitara para sólo ello. No estaba\\nmuy bien con las heridas que don Belianís daba y recebía, porque se\\nimaginaba que, por grandes maestros que le hubiesen curado, no dejaría de\\ntener el rostro y todo el cuerpo lleno de cicatrices y señales. Pero, con\\ntodo, alababa en su autor aquel acabar su libro con la promesa de aquella\\ninacabable aventura, y muchas veces le vino deseo de tomar la pluma y dalle\\nfin al pie de la letra, como allí se promete; y sin duda alguna lo hiciera,\\ny aun saliera con ello, si otros mayores y continuos pensamientos no se lo\\nestorbaran. Tuvo muchas veces competencia con el cura de su lugar —que era\\nhombre docto, graduado en Sigüenza—, sobre cuál había sido mejor caballero:\\nPalmerín de Ingalaterra o Amadís de Gaula; mas maese Nicolás, barbero del\\nmesmo pueblo, decía que ninguno llegaba al Caballero del Febo, y que si\\nalguno se le podía comparar, era don Galaor, hermano de Amadís de Gaula,\\nporque tenía muy acomodada condición para todo; que no era caballero\\nmelindroso, ni tan llorón como su hermano, y que en lo de la valentía no le\\niba en zaga.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["#### Tokenización\n","\n","Empezamos convirtiendo el texto en una lista de palabras.\n","Para ello debemos descargar el módulo `punkt` dentro de NLTK que incluye las funciones de tokenización:"],"metadata":{"id":"2NSF154SQRDn"}},{"cell_type":"code","source":["nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","palabras = word_tokenize(don_quijote)\n","print(type(palabras))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8k3QVL0PoNc","executionInfo":{"status":"ok","timestamp":1709752059572,"user_tz":-60,"elapsed":822,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"6d914988-e4dd-4139-aa2d-c1ab7035b9f5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["<class 'list'>\n"]}]},{"cell_type":"markdown","source":["Observamos que `palabras` es una lista. Véamos sus 20 primeros elementos:"],"metadata":{"id":"mBlcGr2VQ6ul"}},{"cell_type":"code","source":["palabras[0 : 20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_plwB5OQjCR","executionInfo":{"status":"ok","timestamp":1709752073091,"user_tz":-60,"elapsed":3,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"c5fec927-d0d4-45ce-a324-7a7a5801bd82"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Capítulo',\n"," 'primero',\n"," '.',\n"," 'Que',\n"," 'trata',\n"," 'de',\n"," 'la',\n"," 'condición',\n"," 'y',\n"," 'ejercicio',\n"," 'del',\n"," 'famoso',\n"," 'hidalgo',\n"," 'don',\n"," 'Quijote',\n"," 'de',\n"," 'la',\n"," 'Mancha',\n"," 'En',\n"," 'un']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Observamos que los signos de espacio como el del salto de página ya han sido eliminados y que cada palabra ocupa una posición de la lista. Además podemos ver cómo hay palabras repetidas como \"de\" o \"la\" y que los signos de puntuación se tranta como palabras (tenemos un signo de puntuación en la tercera posición).\n","\n","#### Convertimos todas las grafías a minúsculas\n","\n","Así dejaremos de considerar \"Que\" y \"que\" como palabras distintas:"],"metadata":{"id":"b7YyrNBDQ_4j"}},{"cell_type":"code","source":["palabras_minusculas = [palabra.lower() for palabra in palabras]\n","palabras_minusculas[0:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AC5Pk8kyQwhV","executionInfo":{"status":"ok","timestamp":1709752140675,"user_tz":-60,"elapsed":411,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"c1214169-1793-4844-8089-4bbcf8379b18"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['capítulo',\n"," 'primero',\n"," '.',\n"," 'que',\n"," 'trata',\n"," 'de',\n"," 'la',\n"," 'condición',\n"," 'y',\n"," 'ejercicio',\n"," 'del',\n"," 'famoso',\n"," 'hidalgo',\n"," 'don',\n"," 'quijote',\n"," 'de',\n"," 'la',\n"," 'mancha',\n"," 'en',\n"," 'un']"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#### Eliminamos signos de puntuación y caracteres no alfabéticos\n","\n","Elementos como los puntos o las comas no aportan información por lo que serán eliminados:"],"metadata":{"id":"73E_WTU0SBcd"}},{"cell_type":"code","source":["palabras_reales = [palabra for palabra in palabras_minusculas if palabra.isalpha()]\n","palabras_reales[0:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DCZG0jG-R5kn","executionInfo":{"status":"ok","timestamp":1709752190863,"user_tz":-60,"elapsed":4,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"f8953f32-1539-4909-ee70-dcbd561a3ee6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['capítulo',\n"," 'primero',\n"," 'que',\n"," 'trata',\n"," 'de',\n"," 'la',\n"," 'condición',\n"," 'y',\n"," 'ejercicio',\n"," 'del',\n"," 'famoso',\n"," 'hidalgo',\n"," 'don',\n"," 'quijote',\n"," 'de',\n"," 'la',\n"," 'mancha',\n"," 'en',\n"," 'un',\n"," 'lugar']"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["Observamos que caracteres como los puntos han desaparecido.\n","\n","#### Eliminamos palabras vacías\n","\n","No tiene sentido conservar palabras como los artículos o preposiciones que aparecen a lo largo del texto infinidad de veces sin aportar mucha información. Eliminamos estas palabras empleando un módulo preentrenenado de NLTK:"],"metadata":{"id":"MLDH4LLsSb22"}},{"cell_type":"code","source":["nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","palabras_vacias = set(stopwords.words('spanish'))\n","list(palabras_vacias)[0:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnpSrFaqSXiG","executionInfo":{"status":"ok","timestamp":1709752229354,"user_tz":-60,"elapsed":413,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"d538ef82-feb3-480e-e1bf-6df993602fca"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["['hubierais',\n"," 'fueran',\n"," 'estuvieron',\n"," 'vosotros',\n"," 'algo',\n"," 'estoy',\n"," 'hubieras',\n"," 'esas',\n"," 'a',\n"," 'ante']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["len(palabras_vacias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2x2LzJDXLNGK","executionInfo":{"status":"ok","timestamp":1709752252292,"user_tz":-60,"elapsed":3,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"aecb51ca-dc4d-482e-c2f5-b71b3b71f868"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["313"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["palabras_significativas = [palabra for palabra in palabras_reales if palabra not in palabras_vacias]\n","palabras_significativas[0:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zu3dI9OcTEim","executionInfo":{"status":"ok","timestamp":1709752267655,"user_tz":-60,"elapsed":412,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"23048fbe-c492-4c04-eec7-7f0aab59bcc3"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['capítulo',\n"," 'primero',\n"," 'trata',\n"," 'condición',\n"," 'ejercicio',\n"," 'famoso',\n"," 'hidalgo',\n"," 'don',\n"," 'quijote',\n"," 'mancha',\n"," 'lugar',\n"," 'mancha',\n"," 'cuyo',\n"," 'nombre',\n"," 'quiero',\n"," 'acordarme',\n"," 'tiempo',\n"," 'vivía',\n"," 'hidalgo',\n"," 'lanza']"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["### Stemming de las palabras\n","\n","Con este paso del preprocesamiento vamos a hacer que, por ejemplo, todas las formas verbales \"vivir\", \"vivía\" y \"vivirán\" converjan a una única palabra:"],"metadata":{"id":"O6_hfKK_Tu2J"}},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n","stemmer = SnowballStemmer('spanish')\n","palabras_raiz = [stemmer.stem(palabra) for palabra in palabras_significativas]\n","palabras_raiz[0:20]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHOrS4lNTo8v","executionInfo":{"status":"ok","timestamp":1709752323562,"user_tz":-60,"elapsed":361,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"d3d6fb45-a699-4cee-8adb-ebec7222e977"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['capitul',\n"," 'primer',\n"," 'trat',\n"," 'condicion',\n"," 'ejercici',\n"," 'famos',\n"," 'hidalg',\n"," 'don',\n"," 'quijot',\n"," 'manch',\n"," 'lug',\n"," 'manch',\n"," 'cuy',\n"," 'nombr',\n"," 'quier',\n"," 'acord',\n"," 'tiemp',\n"," 'viv',\n"," 'hidalg',\n"," 'lanz']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Observamos como, por ejemplo, la palabra \"Mancha\" pasa tras el preprocesamiento a ser \"manch\" que amalgamaría no solmanete \"Mancha\" y \"mancha\" si no también todas las formas verbales del verbo manchar."],"metadata":{"id":"ScODtGDQUamd"}},{"cell_type":"markdown","source":["## Spacy\n","\n","Veámos un breve ejemplo de las posibilidades de Spacy.\n","\n","Comenzamos cargando el modelo pre-entrenado para español:"],"metadata":{"id":"yFqeX4NwU22E"}},{"cell_type":"code","source":["! python -m spacy download es_core_news_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Lpi9vlUVP71","executionInfo":{"status":"ok","timestamp":1709752372932,"user_tz":-60,"elapsed":17827,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"6f426d70-355e-4446-f958-28ff0b276c73"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting es-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.6.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.16.3)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["import spacy\n","modelo_sp = spacy.load(\"es_core_news_sm\")"],"metadata":{"id":"MC92UiSCUSvN","executionInfo":{"status":"ok","timestamp":1709752394441,"user_tz":-60,"elapsed":5188,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["don_quijote_sp = modelo_sp(don_quijote)"],"metadata":{"id":"L8A4hKbsVIWJ","executionInfo":{"status":"ok","timestamp":1709752405692,"user_tz":-60,"elapsed":322,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["Con estos sencillos pasos ya podemos hacer un análisis morfológico del texto:"],"metadata":{"id":"YSHYBs-zV3px"}},{"cell_type":"code","source":["contador = 0\n","for token in don_quijote_sp:\n","    print(token.text, token.pos_)\n","    contador = contador + 1\n","    if contador > 30:\n","      break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCYijfA0VdAD","executionInfo":{"status":"ok","timestamp":1709752409144,"user_tz":-60,"elapsed":3,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"f528e982-1731-41d8-ad12-f45434594c19"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," SPACE\n","Capítulo VERB\n","primero ADV\n",". PUNCT\n","Que SCONJ\n","trata VERB\n","de ADP\n","la DET\n","condición NOUN\n","y CCONJ\n","ejercicio NOUN\n","del ADP\n","famoso ADJ\n","hidalgo NOUN\n","\n"," SPACE\n","don NOUN\n","Quijote PROPN\n","de ADP\n","la DET\n","Mancha PROPN\n","\n","\n"," SPACE\n","En ADP\n","un DET\n","lugar NOUN\n","de ADP\n","la DET\n","Mancha PROPN\n",", PUNCT\n","de ADP\n","cuyo PRON\n","nombre NOUN\n"]}]},{"cell_type":"markdown","source":["También es posible realizar una extracción de entidades:"],"metadata":{"id":"z_MVOhILV-O5"}},{"cell_type":"code","source":["for ent in don_quijote_sp.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fXnUUK58Vf6W","executionInfo":{"status":"ok","timestamp":1709752476140,"user_tz":-60,"elapsed":331,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"57569b31-84fd-4c5a-e75e-87990b4a76e2"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Capítulo PER\n","hidalgo\n","don Quijote de la Mancha LOC\n","la Mancha LOC\n","Una olla de algo más vaca MISC\n","El resto della concluían sayo de velarte MISC\n","Tenía PER\n","Frisaba LOC\n","Quieren decir MISC\n","Quijada LOC\n","Quesada LOC\n","Quejana PER\n","Pero esto importa MISC\n","año MISC\n","llegó PER\n","Feliciano de Silva PER\n","La razón de la sinrazón MISC\n","Aristóteles PER\n","No MISC\n","Pero PER\n","allí PER\n","Sigüenza LOC\n","Palmerín de Ingalaterra LOC\n","Amadís de Gaula ORG\n","Nicolás LOC\n","Caballero del Febo LOC\n","Galaor PER\n","Amadís PER\n","Gaula PER\n","tenía PER\n","zaga LOC\n"]}]},{"cell_type":"markdown","source":["Observamos que el modelo acierta en muchas ocasiones pero en muchos casos no acaba de afinar. Es importante entender también la dificultad del texto por su antigüedad. Los modelos tienden a funcionar mejor cuando el lenguaje es más popular:"],"metadata":{"id":"mwykfmf1WakN"}},{"cell_type":"code","source":["frase_ejemplo = modelo_sp(\"Donald Trump y Joe Biden se batirán en los próximos comicios en la ciudad de Washington por el gobierno de Estados Unidos\")\n","for ent in frase_ejemplo.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2ZxcuXXWUxN","executionInfo":{"status":"ok","timestamp":1709752535539,"user_tz":-60,"elapsed":534,"user":{"displayName":"Arturo SP","userId":"07770316912620247459"}},"outputId":"ec91171f-61d6-43db-b7c6-d42b5af1bb18"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Donald Trump PER\n","Joe Biden PER\n","Washington LOC\n","Estados Unidos LOC\n"]}]},{"cell_type":"markdown","source":["## Conclusiones\n","\n","En este cuaderno hemos explorado brevemente los módulos `NLTK` y `Spacy`  y hemos visto cómo apoyándonos en ellos es sencillo realizar el preprocesamiento de un texto."],"metadata":{"id":"DrrAp_hbW3Z2"}}]}